# -*- coding: utf-8 -*-
"""summarizationv2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v-cuk4gpKMDlBDgbNFyxuElFpwKTE1ok

#**NOTE**

####Set the Runtime to Use a GPU
1. Navigate to the **Runtime** menu at the top of the Colab interface.
2. Click on **`Runtime`** > **`Change runtime type`**.
3. In the dialog box, under **`Hardware accelerator`**, select **`GPU`** from the dropdown menu.
4. Click **`Save`**.

####Accessing the Files Section
1. To open the **Files** section, click on the **folder icon** in the left sidebar.
2. This will display a file explorer with your current working directory and its contents.

# Summarize PDF Text in Google Colab Using `sshleifer/distilbart-cnn-12-6` Model

In this notebook, we'll extract text from a PDF, break it into manageable chunks, and summarize each chunk using the `sshleifer/distilbart-cnn-12-6` model.

####**WORKFLOW:**
- Install necessary libraries.
*Extract text from the PDF.
*Split the text into smaller chunks.
*Use the model to summarize each chunk.
*Display the summaries.

### Step 1: Install Required Libraries
We need the following libraries:
- **transformers**: for using the pre-trained model.
- **PyMuPDF**: for extracting text from the PDF.
- **NLTK**: for sentence tokenization.
- **pygetpapers**: literature search.
- **amilib**: make JQuery Datatable
"""

!pip install transformers[sentencepiece] pymupdf nltk
!pip install pygetpapers
!pip install -- amilib==0.3.9

"""###Step 2: Importing Libraries"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import fitz  # PyMuPDF
import nltk
import re
from IPython.display import HTML, display

"""###Step 3: Initialize the Tokenizer and Model"""

# Initialize the model and tokenizer
checkpoint = "sshleifer/distilbart-cnn-12-6"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

# Download the necessary NLTK tokenizer
nltk.download('punkt_tab')

"""###Step 4:Download research paper using **pygetpaper**

"""

!pygetpapers --query 'A review medicinal and traditional uses on Tulsi plant (Ocimum sanctum L.)' --pdf --limit 3 --output downloaded_file --save_query

"""###Step 5: To create JQuery Datatables for the retrieved articles using **amilib**"""

!amilib HTML --operation DATATABLES --indir downloaded_file

"""###Step 6: Displaying Datatables (HTML output)."""

from IPython.core.display import display, HTML

# Path to the HTML file
html_file_path = '/content/downloaded_file/datatables.html'

# Read the HTML file
with open(html_file_path, 'r', encoding='utf-8') as file:
    html_content = file.read()

# Display the HTML content
display(HTML(html_content))

"""###Step 7: Loading and extracting text from pdf.

"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page_num in range(doc.page_count):
        page = doc.load_page(page_num)
        text += page.get_text()
    return text

# Replace with the path of your uploaded PDF file
pdf_path ="/content/downloaded_file/PMC11678315/fulltext.pdf"
file_content = extract_text_from_pdf(pdf_path)

"""
###Step 8: Split the Text into Chunks
This block splits the sentences into chunks that the model can handle based on its token limit.

"""

# Split the text into sentences using NLTK
sentences = nltk.tokenize.sent_tokenize(file_content)

# Chunk the text based on token limits of the model
max_chunk_length = tokenizer.model_max_length  # Maximum number of tokens the model can handle
chunks = []
length = 0
chunk = ""

for sentence in sentences:
    sentence_length = len(tokenizer.tokenize(sentence))
    if length + sentence_length <= max_chunk_length:
        chunk += sentence + " "
        length += sentence_length
    else:
        chunks.append(chunk.strip())
        chunk = sentence + " "
        length = sentence_length

# Add the last chunk if there's any left
if chunk:
    chunks.append(chunk.strip())

"""###Step 9:Generate Summaries for Each Chunk"""

# Set a maximum output length for the summary
max_output_length = 50  # You can adjust this value

# Generate summaries for each chunk
summaries = []
for chunk in chunks:
    inputs = tokenizer(chunk, return_tensors="pt", truncation=True, padding=True)
    output = model.generate(
        **inputs,
        max_length=max_output_length,
        min_length=10,
        length_penalty=0.8,
        early_stopping=True,
        no_repeat_ngram_size=3  # Ensures less repetition
    )
    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)

     # Remove standalone numbers and decimals
    cleaned_output = re.sub(r'\b(?:figure|fig)\s*\d+', '', decoded_output)

    cleaned_output = re.sub(r'\[\d+(?:,\d+)*\]', '', cleaned_output)
    cleaned_output = re.sub(r'\d+(\.\d+)+', '', cleaned_output)

    summaries.append(cleaned_output.strip())

"""###Step 10:Display the Final Summary"""

# Concatenate all summaries into one paragraph
final_summary = " ".join(summaries)

# Function to display the final summary vertically
def display_summary_vertically(summary, line_width=90):
    print("\n".join([summary[i:i+line_width] for i in range(0, len(summary), line_width)]))

# Display the final summary
display_summary_vertically(final_summary)