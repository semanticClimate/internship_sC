# -*- coding: utf-8 -*-
"""AI-Assisted_Literature-Review_ENTITY_EXTRACTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qC6OG0-8iJwaQEsg4rDu_QDSlVbiL7RP

**Before using this Colab, please save a copy to your own Google Drive: Click on ‚ÄúFile‚Äù > ‚ÄúSave a copy in Drive‚Äù**
#<font size="6" color="#3498DB">
Entity Extraction Framework for Scholarly Knowledge
</font></center>


# **AI Assisted Literature Review Part III**
# *A. Download Research Papers of interest*
# *B. Demo: PDF to TEXT extraction and Named Entity Recognition**



This **Colab notebook** empowers researchers with a powerful toolkit for extracting key entities from **multiple research articles** with a single click. It leverages various APIs and natural language processing techniques to identify essential information.

#**NOTES**

#### **Set the Runtime to Use GPU**
1. Navigate to the **Runtime** menu at the ribbon of the Colab interface.
2. Click on **`Runtime`** > **`Change runtime type`**.
3. In the dialog box, under **`Hardware accelerator`**, select **`GPU`** from the dropdown menu.
4. Click **`Save`**.

#### **Accessing the Files Section**
1. To open the **Files** section, click on the **folder icon** in the sidebar.
2. This will display a file explorer with your current working directory and its contents.

### **Time-taken:approx 3-4 min**
Summarization Completion Time May Vary depending on:
*  Input Length
*  Model Size
* Hardware(CPU vs GPU)
* Internet stability

**WORKFLOW:**
* Install necessary library to download papers
* Download research paper using *pygetpapers*
* Install necessary library for NER
* Choose the entity for NER (Geolocation, Species, Disease)
* Text extraction from PDFs and extract entities of Interest
* Pearson Correlation within entity
* Create Knowledge Graph

##**Step 1: Install necessary library to download papers**
"""

!pip install pygetpapers

"""##**Step 2: Download research papers using `pygetpapers`.**
User input (title or keywords) to fetch the paper required.**
<br> change the name of the --output folder when you change the query
<br>

"""

!pygetpapers --query '"lung cancer"' --xml --pdf --limit 12 --output disease --save_query

"""##**Step 3: Necessary package installation for NER**"""

# Install core dependencies
!pip install -q spacy beautifulsoup4 nltk tqdm pymupdf babel pandas rapidfuzz

# Upgrade essential tools
!pip install -U pip setuptools wheel numpy

# Install and download spaCy models
!pip install -U spacy
!python -m spacy download en_core_web_lg

# Install scispaCy and scientific models
!pip install -q scispacy
!pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz
!pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz

# Download Disease Ontology file (DOID)
!wget -q -O doid.obo http://purl.obolibrary.org/obo/doid.obo
!pip install WordCloud
!pip install seaborn matplotlib
# Restart the runtime (for changes to take effect)
import os
os.kill(os.getpid(), 9)

#NOTE: Do not Restart the runtime, Click on CANCEL and move to next cell once the installations are done.

"""##**Step 4: Importing the required modules**"""

import os
import re
import fitz  # PyMuPDF
import spacy
import nltk
import requests
import pandas as pd
from tqdm import tqdm
from nltk.tokenize import sent_tokenize
from collections import Counter, defaultdict
from babel import Locale
from IPython.display import display, HTML
from rapidfuzz import process
from functools import lru_cache
from wordcloud import WordCloud
import matplotlib.pyplot as plt

"""##**Step 5: Entity Extraction**

<br>

**<font size="4" color="#2E86C1">Navigate to Entity Extraction Module of your choice:</font>**

*   [Geolocation Extraction](#geolocation)
*   [Species Extraction](#species)
*   [Disease Extraction](#disease)
*   [Gene/Gene Product Extraction](#gene)

<br>

## <a name="geolocation"></a> Geolocation Extraction

##**Geolocation (Country) extraction and validation**
"""

#Download NLTK tokenizer
nltk.download('punkt')
nltk.download('punkt_tab')

#Load spaCy model
nlp = spacy.load("en_core_web_lg")

folder_path = "/content/disease"  # Your PDF folder

#Helper class to extract COUNTRY names (GPEs)
class CountryExtractor:
    def __init__(self):
        self.global_counts = Counter()
        self.per_pdf_counts = defaultdict(Counter)
        self.all_countries_list = set(Locale('en').territories.values())

    def is_country(self, text):
        return text.strip() in self.all_countries_list

    def extract_from_pdf(self, text, pmcid):
        local_counter = Counter()
        sentences = sent_tokenize(text)
        for sentence in sentences:
            doc = nlp(sentence)
            for ent in doc.ents:
                if ent.label_ == "GPE":
                    country = ent.text.strip()
                    if self.is_country(country):
                        local_counter[country] += 1
                        self.global_counts[country] += 1
        self.per_pdf_counts[pmcid] = local_counter

    def get_combined_dataframe(self):
        df = pd.DataFrame(self.global_counts.items(), columns=["Country", "Frequency"])
        for pmcid, counts in self.per_pdf_counts.items():
            df[pmcid] = df["Country"].map(lambda x: counts.get(x, 0))
        df = df.sort_values(by="Frequency", ascending=False).reset_index(drop=True)
        df.index += 1  # Start index from 1
        return df

#Extract text from PDF (excluding references)
def extract_text_without_references(pdf_path):
    full_text = ""
    with fitz.open(pdf_path) as doc:
        for page in doc:
            full_text += page.get_text()
    patterns = [
        r'\nReferences\n.*',
        r'\nREFERENCES\n.*',
        r'\nBibliography\n.*',
    ]
    for pattern in patterns:
        full_text = re.split(pattern, full_text, flags=re.DOTALL)[0]
    return full_text.strip()

#Locate PDFs from nested folders
def get_pdf_files(folder_path):
    pdf_files = []
    for dirpath, _, filenames in os.walk(folder_path):
        for filename in filenames:
            if filename.lower() == "fulltext.pdf":
                pdf_files.append(os.path.join(dirpath, filename))
    return pdf_files

pdf_files = get_pdf_files(folder_path)
print(f"‚úÖ Found {len(pdf_files)} PDFs.")

extractor = CountryExtractor()

print("\nüåç Extracting countries from PDFs...")
for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
    text = extract_text_without_references(pdf_file)
    if len(text) < 100:
        continue
    pmcid = os.path.basename(os.path.dirname(pdf_file))
    extractor.extract_from_pdf(text, pmcid)

#Create DataFrames
df_all = extractor.get_combined_dataframe()
df_all.to_csv("country_frequency_with_pmc.csv")

# ‚úÖ Top 10 summary
df_top10_main = df_all[["Country", "Frequency"]].head(10)
df_top10_full = df_all.head(10)

# ‚úÖ HTML display with expandable per-PDF columns
html = f"""
<style>
    .collapsible {{ background-color: #4CAF50; color: white; cursor: pointer; padding: 10px; width: 100%; border: none; text-align: left; outline: none; font-size: 16px; border-radius: 5px; }}
    .active, .collapsible:hover {{ background-color: #45a049; }}
    .content {{ display: none; padding: 10px 0; margin-top: 10px; }}
    .content table {{ width: 100%; border-collapse: collapse; }}
    .content th, .content td {{ border: 1px solid #ccc; padding: 5px; text-align: left; }}
</style>

{df_top10_main.to_html(index=True)}

<button class="collapsible">‚ñ∂ Expand to view per-PDF country mentions</button>
<div class="content">
{df_top10_full.to_html(index=True)}
</div>

<script>
    var coll = document.querySelector(".collapsible");
    var content = document.querySelector(".content");
    coll.onclick = function() {{
        if (content.style.display === "block") {{
            content.style.display = "none";
            coll.innerHTML = "‚ñ∂ Expand to view per-PDF country mentions";
        }} else {{
            content.style.display = "block";
            coll.innerHTML = "‚ñº Hide per-PDF country mentions";
        }}
    }};
</script>
"""

# ‚úÖ Show HTML output
display(HTML(html))

"""##**Visualise Country Frequencies with Word Cloud**"""

# Replace 'country_frequency.csv' with the actual filename if it's different
df = pd.read_csv('/content/country_frequency_with_pmc.csv')

# Convert the DataFrame into a dictionary: {Country: Frequency}
freq_dict = dict(zip(df['Country'], df['Frequency']))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dict)

# Display the word cloud
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Country Frequency Word Cloud', fontsize=16)
plt.show()

"""##**Co-occurrence Plot of Countries**"""

!pip install pandas seaborn matplotlib
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the CSV
file_path = 'country_frequency_with_pmc.csv'  # Update with your actual CSV path
df = pd.read_csv(file_path)

# Extract only the country and PMCID columns (skip total frequency)
country_names = df['Country']
frequency_matrix = df.iloc[:, 2:]

# Set the index to country names for correlation
frequency_matrix.index = country_names

# Compute Pearson correlation between countries
correlation_matrix = frequency_matrix.T.corr(method='pearson')

# Plotting heatmap with gridlines
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='Blues', square=True,
            linewidths=0.5, linecolor='gray',
            cbar_kws={'label': 'Pearson Correlation Coefficient'})
plt.title('Country Co-occurrence (Pearson Correlation of Frequency Across Papers)')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""## <a name="species"></a> Species Extraction

##**Species extraction and validation**
"""

#NLTK setup
nltk.download("punkt")
nltk.download('punkt_tab')


#Load SciSpaCy model
nlp = spacy.load("en_core_sci_md")

folder_path = "/content/disease"  # Replace with your folder path

#GBIF species validation
def is_valid_species_gbif(name):
    try:
        response = requests.get(
            "https://api.gbif.org/v1/species/match",
            params={"name": name},
            timeout=5
        )
        if response.status_code == 200:
            data = response.json()
            return data.get("matchType") == "EXACT" and data.get("rank") == "SPECIES"
        return False
    except Exception as e:
        print(f"GBIF error for '{name}': {e}")
        return False

#Species extractor
class SpeciesExtractor:
    def __init__(self):
        self.global_counts = Counter()
        self.per_pdf_counts = defaultdict(Counter)

    def is_species_format(self, text):
        return bool(re.match(r"^[A-Z][a-z]+ [a-z]+$", text.strip()))

    def extract_from_pdf(self, text, pmcid):
        local_counter = Counter()
        sentences = sent_tokenize(text)
        for sentence in sentences:
            doc = nlp(sentence)
            for ent in doc.ents:
                name = ent.text.strip()
                if self.is_species_format(name) and is_valid_species_gbif(name):
                    local_counter[name] += 1
                    self.global_counts[name] += 1
        self.per_pdf_counts[pmcid] = local_counter

    def get_combined_dataframe(self):
        df = pd.DataFrame(self.global_counts.items(), columns=["Species", "Frequency"])
        for pmcid, counts in self.per_pdf_counts.items():
            df[pmcid] = df["Species"].map(lambda x: counts.get(x, 0))
        df = df.sort_values(by="Frequency", ascending=False).reset_index(drop=True)
        df.index += 1
        return df

#PDF text extraction without references
def extract_text_without_references(pdf_path):
    full_text = ""
    with fitz.open(pdf_path) as doc:
        for page in doc:
            full_text += page.get_text()
    patterns = [r'\nReferences\n.*', r'\nREFERENCES\n.*', r'\nBibliography\n.*']
    for pattern in patterns:
        full_text = re.split(pattern, full_text, flags=re.DOTALL)[0]
    return full_text.strip()

# ‚úÖ Step 8: Get all PDFs from folder
def get_pdf_files(folder_path):
    pdf_files = []
    for dirpath, _, filenames in os.walk(folder_path):
        for filename in filenames:
            if filename.lower() == "fulltext.pdf":
                pdf_files.append(os.path.join(dirpath, filename))
    return pdf_files

#Run extraction

pdf_files = get_pdf_files(folder_path)

print(f"‚úÖ Found {len(pdf_files)} PDFs.")

extractor = SpeciesExtractor()

print("\nüß¨ Extracting species (validated via GBIF)...")
for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
    text = extract_text_without_references(pdf_file)
    if len(text) < 100:
        continue
    pmcid = os.path.basename(os.path.dirname(pdf_file))
    extractor.extract_from_pdf(text, pmcid)

#Generate tables
df_all = extractor.get_combined_dataframe()
df_all.to_csv("species_frequency_with_pmc.csv")

df_top10_main = df_all[["Species", "Frequency"]].head(10)
df_top10_full = df_all.head(10)

#Expandable HTML display
html = f"""
<style>
    .collapsible {{ background-color: #4CAF50; color: white; cursor: pointer; padding: 10px; width: 100%; border: none; text-align: left; outline: none; font-size: 16px; border-radius: 5px; }}
    .active, .collapsible:hover {{ background-color: #45a049; }}
    .content {{ display: none; padding: 10px 0; margin-top: 10px; }}
    .content table {{ width: 100%; border-collapse: collapse; }}
    .content th, .content td {{ border: 1px solid #ccc; padding: 5px; text-align: left; }}
</style>

{df_top10_main.to_html(index=True)}

<button class="collapsible">‚ñ∂ Expand to view per-PDF species mentions</button>
<div class="content">
{df_top10_full.to_html(index=True)}
</div>

<script>
    var coll = document.querySelector(".collapsible");
    var content = document.querySelector(".content");
    coll.onclick = function() {{
        if (content.style.display === "block") {{
            content.style.display = "none";
            coll.innerHTML = "‚ñ∂ Expand to view per-PDF species mentions";
        }} else {{
            content.style.display = "block";
            coll.innerHTML = "‚ñº Hide per-PDF species mentions";
        }}
    }};
</script>
"""

#Display results
display(HTML(html))

"""##**Visualise Species Frequencies with Word Cloud**"""

# Replace 'species_frequency_with_pmc.csv' with the actual filename if it's different
df = pd.read_csv('/content/species_frequency_with_pmc.csv')

# Convert the DataFrame into a dictionary: {Species: Frequency}
freq_dict = dict(zip(df['Species'], df['Frequency']))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dict)

# Display the word cloud
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Species Frequency Word Cloud', fontsize=16)
plt.show()

"""##**Co-occurrence Plot of Species**"""

!pip install pandas seaborn matplotlib
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the CSV
file_path = 'species_frequency_with_pmc.csv'  # Update with your actual CSV path
df = pd.read_csv(file_path)

# Extract only the species and PMCID columns (skip total frequency)
species_names = df['Species']
frequency_matrix = df.iloc[:, 2:]

# Set the index to species names for correlation
frequency_matrix.index = species_names

# Compute Pearson correlation between species
correlation_matrix = frequency_matrix.T.corr(method='pearson')

# Plotting heatmap with gridlines
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='Blues', square=True,
            linewidths=0.5, linecolor='gray',
            cbar_kws={'label': 'Pearson Correlation Coefficient'})
plt.title('Species Co-occurrence (Pearson Correlation of Frequency Across Papers)')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""## <a name="disease"></a> Disease Extraction

##**Disease extraction and validation**
"""

folder_path = "/content/disease"  # Change to your PDF folder path

#Load Disease Ontology terms from doid.obo
def load_disease_terms(obo_path):
    diseases = set()
    with open(obo_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith('name: '):
                diseases.add(line.strip().split('name: ')[1].lower())
            elif line.startswith('synonym: '):
                parts = line.strip().split('"')
                if len(parts) > 1:
                    diseases.add(parts[1].lower())
    return diseases

DISEASE_TERMS = load_disease_terms('doid.obo')

# Stopwords and blacklist to filter common false positives
STOPWORDS = set([
    "disease", "syndrome", "disorder", "infection","virus", "human",
    "condition", "patient", "treatment", "therapy", "symptom", "clinical",
    "case", "effect", "study", "result", "analysis", "group", "control"
])

def clean_entity(ent):
    ent = ent.lower().strip()
    if len(ent) < 3:
        return None
    if any(char.isdigit() for char in ent):
        return None
    if re.search(r'[^a-z\s\-]', ent):  # allow letters, spaces, hyphens only
        return None
    if ent in STOPWORDS:
        return None
    return ent

# Cache fuzzy matching results for speed
@lru_cache(maxsize=10000)
def cached_fuzzy_match(name):
    match = process.extractOne(name, DISEASE_TERMS, score_cutoff=95)
    return match is not None

def is_valid_disease(name):
    name_lower = name.lower()
    if name_lower in DISEASE_TERMS:
        return True
    return cached_fuzzy_match(name_lower)

# Load SciSpaCy disease model
nlp = spacy.load("en_ner_bc5cdr_md")

class DiseaseExtractor:
    def __init__(self):
        self.global_counts = Counter()
        self.per_pdf_counts = defaultdict(Counter)

    def extract_from_pdf(self, text, pmcid):
        local_counter = Counter()
        # Clean text
        text = re.sub(r'[^\x00-\x7F]+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        doc = nlp(text)
        # Filter sentences longer than 10 chars
        sentences = [sent.text for sent in doc.sents if len(sent.text) > 10]

        # Batch process sentences
        docs = list(nlp.pipe(sentences, batch_size=50))
        for doc in docs:
            for ent in doc.ents:
                if ent.label_ == "DISEASE":
                    disease = clean_entity(ent.text)
                    if disease and is_valid_disease(disease):
                        local_counter[disease] += 1
                        self.global_counts[disease] += 1

        # Filter entities mentioned more than once per PDF
        filtered_counter = Counter({k: v for k, v in local_counter.items() if v > 1})
        self.per_pdf_counts[pmcid] = filtered_counter

    def get_combined_dataframe(self):
        df = pd.DataFrame(self.global_counts.items(), columns=["Disease", "Frequency"])
        for pmcid, counts in self.per_pdf_counts.items():
            df[pmcid] = df["Disease"].map(lambda x: counts.get(x, 0))
        df = df[df["Frequency"] > 1]  # global frequency filter
        df = df.sort_values(by="Frequency", ascending=False).reset_index(drop=True)
        df.index += 1
        return df

def extract_text_without_references(pdf_path):
    full_text = ""
    with fitz.open(pdf_path) as doc:
        for page in doc:
            full_text += page.get_text()
    patterns = [r'\nReferences\n.*', r'\nREFERENCES\n.*', r'\nBibliography\n.*']
    for pattern in patterns:
        full_text = re.split(pattern, full_text, flags=re.DOTALL)[0]
    return full_text.strip()

def get_pdf_files(folder_path):
    pdf_files = []
    for dirpath, _, filenames in os.walk(folder_path):
        for filename in filenames:
            if filename.lower() == "fulltext.pdf":
                pdf_files.append(os.path.join(dirpath, filename))
    return pdf_files

# === Run extraction ===

pdf_files = get_pdf_files(folder_path)


extractor = DiseaseExtractor()

print("\nü¶† Extracting diseases (optimized)...")
for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
    text = extract_text_without_references(pdf_file)
    if len(text) < 100:
        continue
    pmcid = os.path.basename(os.path.dirname(pdf_file))
    extractor.extract_from_pdf(text, pmcid)

df_all = extractor.get_combined_dataframe()
df_all.to_csv("disease_frequency_with_pmc.csv")

df_top10_main = df_all[["Disease", "Frequency"]].head(10)
df_top10_full = df_all.head(10)

html = f"""
<style>
    .collapsible {{ background-color: #4CAF50; color: white; cursor: pointer; padding: 10px; width: 100%; border: none; text-align: left; outline: none; font-size: 16px; border-radius: 5px; }}
    .active, .collapsible:hover {{ background-color: #45a049; }}
    .content {{ display: none; padding: 10px 0; margin-top: 10px; }}
    .content table {{ width: 100%; border-collapse: collapse; }}
    .content th, .content td {{ border: 1px solid #ccc; padding: 5px; text-align: left; }}
</style>

{df_top10_main.to_html(index=True)}

<button class="collapsible">‚ñ∂ Expand to view per-PDF disease mentions</button>
<div class="content">

{df_top10_full.to_html(index=True)}
</div>

<script>
    var coll = document.querySelector(".collapsible");
    var content = document.querySelector(".content");
    coll.onclick = function() {{
        if (content.style.display === "block") {{
            content.style.display = "none";
            coll.innerHTML = "‚ñ∂ Expand to view per-PDF disease mentions";
        }} else {{
            content.style.display = "block";
            coll.innerHTML = "‚ñº Hide per-PDF disease mentions";
        }}
    }};
</script>
"""

display(HTML(html))

"""##**Visualise Disease Frequencies with Word Cloud**"""

# Replace 'disease_frequency_with_pmc.csv' with the actual filename if it's different
df = pd.read_csv('/content/disease_frequency_with_pmc.csv')

# Convert the DataFrame into a dictionary: {Species: Frequency}
freq_dict = dict(zip(df['Disease'], df['Frequency']))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dict)

# Display the word cloud
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Disease Frequency Word Cloud', fontsize=16)
plt.show()

"""##**Co-occurrence Plot of Disease**"""

!pip install pandas seaborn matplotlib
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the CSV
file_path = 'disease_frequency_with_pmc.csv'  # Update with your actual CSV path
df = pd.read_csv(file_path)

# Extract only the disease and PMCID columns (skip total frequency)
disease_names = df['Disease']
frequency_matrix = df.iloc[:, 2:]

# Set the index to disease names for correlation
frequency_matrix.index = disease_names

# Compute Pearson correlation between diseases
correlation_matrix = frequency_matrix.T.corr(method='pearson')

# Plotting heatmap with gridlines
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='Blues', square=True,
            linewidths=0.5, linecolor='gray',
            cbar_kws={'label': 'Pearson Correlation Coefficient'})
plt.title('Disease Co-occurrence (Pearson Correlation of Frequency Across Papers)')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""## <a name="gene"></a> Gene/Gene Product Extraction

##**Gene/Gene Product extraction and validation**
"""

# ‚îÄ‚îÄ USER CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PDF_FOLDER = "/content/disease"        # Change this to your root PDF directory
HGNC_TSV    = "hgnc_complete_set.txt"  # HGNC download target
SYMBOLS_TXT = "hgnc_gene_symbols.txt"  # Processed one-symbol-per-line
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# 1. Download & prepare HGNC list if missing
# Updated URL to avoid 404
HGNC_URL = "https://storage.googleapis.com/public-download-files/hgnc/tsv/tsv/hgnc_complete_set.txt"

if not os.path.exists(HGNC_TSV):
    print("Downloading HGNC data‚Ä¶")
    resp = requests.get(HGNC_URL)
    resp.raise_for_status()
    with open(HGNC_TSV, "wb") as f:
        f.write(resp.content)

if not os.path.exists(SYMBOLS_TXT):
    print("Extracting HGNC symbols‚Ä¶")
    df_hgnc = pd.read_csv(HGNC_TSV, sep="\t", usecols=["symbol"])
    df_hgnc["symbol"].str.lower()\
        .dropna()\
        .sort_values()\
        .drop_duplicates()\
        .to_csv(SYMBOLS_TXT, index=False, header=False)

# 2. Load HGNC gene symbols into a set
def load_gene_symbols(path):
    with open(path, 'r', encoding='utf-8') as f:
        return set(sym.strip().lower() for sym in f if sym.strip())

GENE_SYMBOLS = load_gene_symbols(SYMBOLS_TXT)

# 3. Stopwords & cleaning for gene mentions
GENE_STOPWORDS = {
    "expression","level","protein","activity","sequence","region",
    "gene","mutant","domain","mutation","sample","cell","binding",
    "activation","target","site"
}

def clean_gene_entity(ent: str):
    e = ent.lower().strip()
    if len(e) < 2:                   return None
    if any(ch.isdigit() for ch in e):return None
    if re.search(r'[^a-z\-]', e):    return None
    if e in GENE_STOPWORDS:          return None
    return e

@lru_cache(maxsize=10000)
def cached_gene_match(name: str) -> bool:
    return process.extractOne(name, GENE_SYMBOLS, score_cutoff=95) is not None

def is_valid_gene(name: str) -> bool:
    nl = name.lower()
    return nl in GENE_SYMBOLS or cached_gene_match(nl)

# 4. Load SciSpaCy gene NER model
nlp_gene = spacy.load("en_ner_bionlp13cg_md")

# 5. PDF‚Üítext (strip References/Bib)
def extract_text_without_references(pdf_path):
    txt = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                txt += page.get_text()
        for pat in (r'\nReferences\n.*', r'\nREFERENCES\n.*', r'\nBibliography\n.*'):
            txt = re.split(pat, txt, flags=re.DOTALL)[0]
    except Exception as e:
        print(f"‚ö†Ô∏è Skipping {pdf_path} due to error: {e}")
        return None
    return txt.strip()

# 6. Find all ‚Äúfulltext.pdf‚Äù under PDF_FOLDER
def get_pdf_files(root):
    files = []
    for dp, _, fns in os.walk(root):
        for fn in fns:
            if fn.lower() == "fulltext.pdf":
                files.append(os.path.join(dp, fn))
    return files

# 7. GeneExtractor class (mirrors your disease script)
class GeneExtractor:
    def __init__(self):
        self.global_counts = Counter()
        self.per_pdf_counts = defaultdict(Counter)

    def extract_from_text(self, text, doc_id):
        local = Counter()
        # normalize whitespace
        txt = re.sub(r'[^\x00-\x7F]+',' ', text)
        txt = re.sub(r'\s+',' ', txt)
        doc = nlp_gene(txt)
        sents = [s.text for s in doc.sents if len(s.text)>10]
        for subdoc in nlp_gene.pipe(sents, batch_size=50):
            for ent in subdoc.ents:
                if ent.label_ == "GENE_OR_GENE_PRODUCT":
                    g = clean_gene_entity(ent.text)
                    if g and is_valid_gene(g):
                        local[g] += 1
                        self.global_counts[g] += 1
        # keep only genes with >1 mention in that PDF
        self.per_pdf_counts[doc_id] = Counter({g:v for g,v in local.items() if v>1})

    def get_dataframe(self):
        df = pd.DataFrame(self.global_counts.items(), columns=["Gene","Frequency"])
        for doc_id, ctr in self.per_pdf_counts.items():
            df[doc_id] = df["Gene"].map(lambda g: ctr.get(g, 0))
        df = (df[df["Frequency"]>1]
                .sort_values("Frequency", ascending=False)
                .reset_index(drop=True))
        df.index += 1
        return df

# ‚îÄ‚îÄ RUN THE EXTRACTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
pdfs = get_pdf_files(PDF_FOLDER)
extractor = GeneExtractor()

print("üß¨ Extracting genes from PDFs‚Ä¶")
for fn in tqdm(pdfs, desc="PDFs"):
    text = extract_text_without_references(fn)
    if not text or len(text) < 100:
        continue  # skip empty or problematic PDFs
    doc_id = os.path.basename(os.path.dirname(fn))
    extractor.extract_from_text(text, doc_id)

# build & save results
df = extractor.get_dataframe()
df.to_csv("gene_frequency_with_pmc.csv", index=False)

# render top-10 + collapsible breakdown as HTML in notebook
top10      = df[["Gene","Frequency"]].head(10)
top10_full = df.head(10)
html = f"""
<style>
  .collapsible {{ background:#2196F3; color:#fff; padding:8px; border:none;
                  width:100%; text-align:left; font-size:15px; border-radius:4px; }}
  .active, .collapsible:hover {{ background:#1976D2; }}
  .content {{ display:none; margin-top:8px; }}
  .content table {{ width:100%; border-collapse:collapse; }}
  .content th, .content td {{ border:1px solid #ccc; padding:4px; text-align:left; }}
</style>
{top10.to_html(index=True)}
<button class="collapsible">‚ñ∂ Expand per-PDF breakdown</button>
<div class="content">
{top10_full.to_html(index=True)}
</div>
<script>
  var coll = document.querySelector(".collapsible");
  var cnt  = document.querySelector(".content");
  coll.onclick = function() {{
    if (cnt.style.display === "block") {{
      cnt.style.display = "none";
      coll.textContent = "‚ñ∂ Expand per-PDF breakdown";
    }} else {{
      cnt.style.display = "block";
      coll.textContent = "‚ñº Hide per-PDF breakdown";
    }}
  }};</script>
"""
display(HTML(html))

"""##**Visualise Gene/Gene Product with Word Cloud**"""

# Replace 'disease_frequency_with_pmc.csv' with the actual filename if it's different
df = pd.read_csv('/content/gene_frequency_with_pmc.csv')

# Convert the DataFrame into a dictionary: {Species: Frequency}
freq_dict = dict(zip(df['Gene'], df['Frequency']))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dict)

# Display the word cloud
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Gene/Gene product Frequency Word Cloud', fontsize=16)
plt.show()

"""##**Co-occurrence Plot of Gene/Gene Product**"""

!pip install pandas seaborn matplotlib
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the CSV
file_path = 'gene_frequency_with_pmc.csv'  # Update with your actual CSV path
df = pd.read_csv(file_path)

# Extract only the gene and PMCID columns (skip total frequency)
gene_names = df['Gene']
frequency_matrix = df.iloc[:, 2:]

# Set the index to gene names for correlation
frequency_matrix.index = gene_names

# Compute Pearson correlation between genes
correlation_matrix = frequency_matrix.T.corr(method='pearson')

# Plotting heatmap with gridlines
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='Blues', square=True,
            linewidths=0.5, linecolor='gray',
            cbar_kws={'label': 'Pearson Correlation Coefficient'})
plt.title('Gene/Gene Product Co-occurrence (Pearson Correlation of Frequency Across Papers)')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""##**Step 9: Network of Countries, Species, Diseases and Genes/Gene Products**"""

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

def create_knowledge_graph(country_csv_path, species_csv_path, disease_csv_path, gene_csv_path):
    """
    Creates a knowledge graph with PMCIDs as central nodes,
    connected to related countries, species, diseases, and genes.
    Entities in the same paper are connected together, with weighted edges.
    """

    # Read the CSV files
    countries_df = pd.read_csv(country_csv_path)
    species_df = pd.read_csv(species_csv_path)
    diseases_df = pd.read_csv(disease_csv_path)
    genes_df = pd.read_csv(gene_csv_path)

    # Create the graph
    graph = nx.Graph()

    # Get PMC columns
    pmc_cols_country = countries_df.columns[3:]
    pmc_cols_species = species_df.columns[3:]
    pmc_cols_disease = diseases_df.columns[3:]
    pmc_cols_gene = genes_df.columns[3:]

    # Add nodes for entities
    for country in countries_df['Country']:
        graph.add_node(country, type='country')

    for species in species_df['Species']:
        graph.add_node(species, type='species')

    for disease in diseases_df['Disease']:
        graph.add_node(disease, type='disease')

    for gene in genes_df['Gene']:
        graph.add_node(gene, type='gene')

    # Union of all PMC IDs from the datasets
    all_pmc_cols = set(pmc_cols_country) | set(pmc_cols_species) | set(pmc_cols_disease) | set(pmc_cols_gene)

    for pmc in all_pmc_cols:
        graph.add_node(pmc, type='pmcid')

        # Connect countries
        for _, row in countries_df.iterrows():
            country = row['Country']
            weight = row.get(pmc, 0)
            if weight > 0:
                graph.add_edge(pmc, country, type='mentions', weight=weight)

        # Connect species
        for _, row in species_df.iterrows():
            species = row['Species']
            weight = row.get(pmc, 0)
            if weight > 0:
                graph.add_edge(pmc, species, type='mentions', weight=weight)

        # Connect diseases
        for _, row in diseases_df.iterrows():
            disease = row['Disease']
            weight = row.get(pmc, 0)
            if weight > 0:
                graph.add_edge(pmc, disease, type='mentions', weight=weight)

        # Connect genes
        for _, row in genes_df.iterrows():
            gene = row['Gene']
            weight = row.get(pmc, 0)
            if weight > 0:
                graph.add_edge(pmc, gene, type='mentions', weight=weight)

    # Remove nodes with no edges
    nodes_to_remove = [node for node, degree in dict(graph.degree()).items() if degree == 0]
    graph.remove_nodes_from(nodes_to_remove)

    return graph

def visualize_knowledge_graph(graph):
    """
    Visualizes the knowledge graph with different colors and all nodes as circles,
    a circular legend with readable text, and edge width based on weight.
    """
    # Separate nodes by type
    country_nodes = [node for node, data in graph.nodes(data=True) if data['type'] == 'country']
    species_nodes = [node for node, data in graph.nodes(data=True) if data['type'] == 'species']
    disease_nodes = [node for node, data in graph.nodes(data=True) if data['type'] == 'disease']
    gene_nodes = [node for node, data in graph.nodes(data=True) if data['type'] == 'gene']
    pmcid_nodes = [node for node, data in graph.nodes(data=True) if data['type'] == 'pmcid']

    # Define layout
    pos = nx.spring_layout(graph, k=0.7, iterations=50)

    # Node sizes
    node_size_pmcid = 400
    node_size_entity = 600

    # Draw nodes
    nx.draw_networkx_nodes(graph, pos, nodelist=country_nodes, node_color='skyblue', node_size=node_size_entity)
    nx.draw_networkx_nodes(graph, pos, nodelist=species_nodes, node_color='lightgreen', node_size=node_size_entity)
    nx.draw_networkx_nodes(graph, pos, nodelist=disease_nodes, node_color='lightcoral', node_size=node_size_entity)
    nx.draw_networkx_nodes(graph, pos, nodelist=gene_nodes, node_color='plum', node_size=node_size_entity)
    nx.draw_networkx_nodes(graph, pos, nodelist=pmcid_nodes, node_color='lightgray', node_size=node_size_pmcid)

    # Edge weights
    edges = graph.edges()
    weights = [graph[u][v]['weight'] for u, v in edges]
    if weights:
        max_weight = max(weights)
        normalized_weights = [(w / max_weight) * 2.5 + 0.5 for w in weights]
    else:
        normalized_weights = []

    # Draw edges and labels
    nx.draw_networkx_edges(graph, pos, alpha=0.5, width=normalized_weights)
    nx.draw_networkx_labels(graph, pos, font_size=12)

    # Legend
    country_patch = mpatches.Patch(color='skyblue', label='Countries')
    species_patch = mpatches.Patch(color='lightgreen', label='Species')
    disease_patch = mpatches.Patch(color='lightcoral', label='Diseases')
    gene_patch = mpatches.Patch(color='plum', label='Genes')
    pmcid_patch = mpatches.Patch(color='lightgray', label='PMCIDs')

    plt.legend(handles=[country_patch, species_patch, disease_patch, gene_patch, pmcid_patch],
               loc='center', bbox_to_anchor=(0.5, -0.05), ncol=5, fontsize='large', frameon=False)

    plt.title('Knowledge Graph of Countries, Species, Diseases, Genes, and PMCIDs')
    plt.gcf().set_size_inches(18, 14)
    plt.tight_layout(rect=[0, 0.05, 1, 1])
    plt.show()

def analyze_clusters(graph):
    """
    Performs basic cluster analysis on the network graph.
    """
    components = list(nx.connected_components(graph))
    for i, component in enumerate(components):
        print(f"Cluster {i + 1}: Size = {len(component)}")

# Example usage (update file paths accordingly)
country_csv_path = '/content/country_frequency_with_pmc.csv'
species_csv_path = '/content/species_frequency_with_pmc.csv'
disease_csv_path = '/content/disease_frequency_with_pmc.csv'
gene_csv_path = '/content/gene_frequency_with_pmc.csv'

graph = create_knowledge_graph(country_csv_path, species_csv_path, disease_csv_path, gene_csv_path)
visualize_knowledge_graph(graph)
analyze_clusters(graph)