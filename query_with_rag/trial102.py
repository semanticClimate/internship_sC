# -*- coding: utf-8 -*-
"""trial102.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RteHNh-ZROSSxja7tYRaKVCwT5wWOeVP

## **Query Search with RAG/LLM in Google Colab**

This Colab notebook processes scientific papers, extracts metadata, creates a searchable vector database, and enables interactive question-answering using the Retrieval-Augmented Generation (RAG) approach with Groq's LLM. You can easily query the system for answers based on the processed documents.

### **Step 1: Install dependencies**
* **pymupdf4llm:** Lightweight PDF processing for LLMs
* **langchain:** Framework for developing LLM-powered applications
* **chromadb:** Vector store for storing and querying embeddings
* **sentence-transformers:** For embedding sentences using transformer models
"""

# Install dependencies
!pip install pygetpapers
!pip install lxml
!pip install pymupdf4llm langchain chromadb sentence-transformers
!pip install -U langchain-community langchain-groq

"""### **Step 2: Set Groq API Key**
Groq‚Äôs LPU (Language Processing Unit) hardware enables real-time, low-latency responses from LLMs‚Äîideal for interactive applications.

**Instructions:**
* Go to https://console.groq.com/

* Create an account if you don‚Äôt have one

* Generate your API token

* Copy and paste it when prompted below
"""

#  Set API Key
import os, getpass
os.environ["GROQ_API_KEY"] = getpass.getpass("üîê Enter your Groq API Key: ")

"""### **Step 3: Download Research Papers**
Use *pygetpapers* to fetch research articles related to the keyword.
"""

# Download papers from EuropePMC
!pygetpapers --query '"phytochemical"' --xml --limit 15 --output /content/data_phyto --save_query

"""### **Step 4: Parse XML Files to Markdown and Extract Metadata**
Convert scientific articles (downloaded in XML format) into clean Markdown format and extract essential metadata like title, authors, and DOI.
"""

#  Parse XMLs to Markdown and extract metadata
import pathlib
import re
from lxml import etree

def sanitize_filename(name):
    return re.sub(r'[\/:"*?<>|]+', "_", name)

def parse_xml_to_markdown_with_metadata(xml_path):
    try:
        with open(xml_path, 'rb') as f:
            tree = etree.parse(f)

        metadata = {
            "title": "",
            "authors": [],
            "doi": "",
        }

        # Extract title
        title_elem = tree.find(".//article-title")
        if title_elem is not None:
            full_title = title_elem.xpath("string()")  # ‚úÖ gets entire text including inside nested tags
            metadata["title"] = full_title.strip()


        # Extract DOI
        doi_elem = tree.find(".//article-id[@pub-id-type='doi']")
        if doi_elem is not None and doi_elem.text:
            metadata["doi"] = "https://doi.org/" + doi_elem.text.strip()

        # Extract authors
        authors = []
        for contrib in tree.findall(".//contrib[@contrib-type='author']"):
            name = contrib.find('name')
            if name is not None:
                given = name.findtext('given-names', default='')
                surname = name.findtext('surname', default='')
                full_name = f"{given} {surname}".strip()
                if full_name:
                    authors.append(full_name)

        metadata["authors"] = ", ".join(authors)

        # Extract body content
        sections = tree.xpath('//body//sec')
        text_parts = []

        for sec in sections:
            title = sec.findtext('title')
            if title:
                text_parts.append(f"### {title.strip()}")
            paragraphs = sec.findall('p')
            for p in paragraphs:
                if p.text and p.text.strip():
                    text_parts.append(p.text.strip())

        markdown_text = "\n\n".join(text_parts)
        return markdown_text, metadata

    except Exception as e:
        print(f" Error parsing {xml_path.name}: {e}")
        return None  #  Safe fallback to prevent unpacking error

"""### **Step 5: Process and Convert Scientific XMLs to Markdown**
Automate the batch conversion of multiple scientific XML files to Markdown and collect their metadata.
"""

def process_scientific_xmls(data_directory, output_directory):
    data_path = pathlib.Path(data_directory)
    output_path = pathlib.Path(output_directory)
    output_path.mkdir(parents=True, exist_ok=True)

    metadata_records = []

    xml_files = list(data_path.glob("**/fulltext.xml"))
    for xml_file in xml_files:
        print(f" Processing {xml_file.name} ...")

        #  Skip empty XML files
        if xml_file.stat().st_size == 0:
            print(f" Skipped: {xml_file.name} (Empty file)")
            continue

        #  Safe call and unpack
        result = parse_xml_to_markdown_with_metadata(xml_file)
        if result is None:
            continue
        raw_text, metadata = result

        #  Save Markdown
        sanitized_name = sanitize_filename(xml_file.parent.name)
        final_filename = output_path / f"{sanitized_name}_final.md"

        if raw_text.strip():
            final_filename.write_text(raw_text, encoding="utf-8")
            print(f" Saved: {final_filename.name}")
            metadata["filename"] = final_filename.name
            metadata_records.append((final_filename, metadata))
        else:
            print(f" Skipped: {xml_file.name} (No extractable content)")

    return metadata_records

"""### **Step 6: Load, Chunk, and Store Documents in a Vector Database**
Process documents to store them as vectors, enabling question-answering with a retrieval system.
"""

#  Load and Chunk Documents with Metadata
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq

def load_markdown_documents_with_metadata(metadata_records):
    documents = []
    for md_path, metadata in metadata_records:
        text = md_path.read_text(encoding="utf-8")
        if not text.strip():
            continue
        doc = Document(page_content=text, metadata=metadata)
        documents.append(doc)
    return documents

def hybrid_chunking(documents, threshold=3000):
    chunks = []
    for doc in documents:
        if len(doc.page_content.strip()) <= threshold:
            chunks.append(doc)
        else:
            splitter = RecursiveCharacterTextSplitter(chunk_size=1800, chunk_overlap=300)
            split_docs = splitter.split_documents([doc])
            for chunk in split_docs:
                chunk.metadata.update(doc.metadata)
            chunks.extend(split_docs)
    return chunks

def create_vector_database(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")
    vector_db = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name="scientific_rag_xml",
        persist_directory="/content/db"
    )
    return vector_db

def create_retrieval_chain_with_groq(vector_db):
    llm = ChatGroq(
        model="llama3-70b-8192",
        temperature=0.2,
        max_tokens=512,
        api_key=os.environ.get("GROQ_API_KEY")
    )
    prompt_template = PromptTemplate.from_template(
        '''You are a very good research paper assistant. Use this context to provide the following questions. You can use your knowledge if asked general bio and chemistry related questions.

Context:
{context}

Question: {question}

Answer:'''
    )
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt_template}
    )
    return qa_chain

"""### **Step 7: Execute Full Pipeline for Document Processing and Retrieval**
Run the entire pipeline from downloading scientific papers to processing them, creating a vector database, and setting up the question-answering system.
"""

# ‚úÖ Execute Full Pipeline
pdf_dir = "/content/data_phyto"
markdown_dir = "/content/markdowns"
os.makedirs(markdown_dir, exist_ok=True)

metadata_records = process_scientific_xmls(pdf_dir, markdown_dir)
docs = load_markdown_documents_with_metadata(metadata_records)
chunks = hybrid_chunking(docs)
vector_db = create_vector_database(chunks)
qa_chain = create_retrieval_chain_with_groq(vector_db)
print(" RAG System Ready.")

"""### **Step 8: Query the Retrieval System**
Allow users to ask scientific questions and get answers based on the documents stored in the vector database
"""

from IPython.display import Markdown, display

#  Ask Questions
while True:
    query = input("üß† Ask a scientific question (or type 'quit'): ").strip()
    if query.lower() == "quit":
        break

    result = qa_chain.invoke(query)
    answer = result.get("result", "")

    # Display answer with wrapped formatting
    display(Markdown(f"###  Answer:\n\n{answer}"))

    # Format sources as clickable markdown links
    # Format sources with safe markdown titles
    source_lines = []
    for doc in result['source_documents']:
        title = doc.metadata.get("title", "Untitled")
        title = re.sub(r"[()]", "", title)  # Remove parentheses that break markdown
        doi = doc.metadata.get("doi", "")
        source_lines.append(f"- [{title}]({doi})" if doi else f"- {title}")

    display(Markdown("** Sources:**\n" + "\n".join(source_lines)))

